{
  "version": 3,
  "sources": [],
  "sections": [
    {"offset": {"line": 6, "column": 0}, "map": {"version":3,"sources":[],"names":[],"mappings":"","debugId":null}},
    {"offset": {"line": 60, "column": 0}, "map": {"version":3,"sources":["file:///Users/zeke/projects/highwater/packages/web/src/app/api/ollama-proxy/route.ts"],"sourcesContent":["import { NextRequest, NextResponse } from 'next/server';\n\nexport async function POST(req: NextRequest) {\n  const { prompt } = await req.json();\n  if (!prompt) {\n    return NextResponse.json({ error: 'No prompt provided.' }, { status: 400 });\n  }\n\n  try {\n    const ollamaRes = await fetch('http://127.0.0.1:11434/api/generate', {\n      method: 'POST',\n      headers: { 'Content-Type': 'application/json' },\n      body: JSON.stringify({\n        model: 'llama3.2',\n        prompt,\n        stream: false,\n      }),\n    });\n    const data = await ollamaRes.json();\n    return NextResponse.json(data);\n  } catch (err: any) {\n    return NextResponse.json({ error: err.message || 'Failed to connect to Ollama.' }, { status: 500 });\n  }\n}"],"names":[],"mappings":";;;AAAA;;AAEO,eAAe,KAAK,GAAgB;IACzC,MAAM,EAAE,MAAM,EAAE,GAAG,MAAM,IAAI,IAAI;IACjC,IAAI,CAAC,QAAQ;QACX,OAAO,qOAAA,CAAA,eAAY,CAAC,IAAI,CAAC;YAAE,OAAO;QAAsB,GAAG;YAAE,QAAQ;QAAI;IAC3E;IAEA,IAAI;QACF,MAAM,YAAY,MAAM,MAAM,uCAAuC;YACnE,QAAQ;YACR,SAAS;gBAAE,gBAAgB;YAAmB;YAC9C,MAAM,KAAK,SAAS,CAAC;gBACnB,OAAO;gBACP;gBACA,QAAQ;YACV;QACF;QACA,MAAM,OAAO,MAAM,UAAU,IAAI;QACjC,OAAO,qOAAA,CAAA,eAAY,CAAC,IAAI,CAAC;IAC3B,EAAE,OAAO,KAAU;QACjB,OAAO,qOAAA,CAAA,eAAY,CAAC,IAAI,CAAC;YAAE,OAAO,IAAI,OAAO,IAAI;QAA+B,GAAG;YAAE,QAAQ;QAAI;IACnG;AACF","debugId":null}}]
}